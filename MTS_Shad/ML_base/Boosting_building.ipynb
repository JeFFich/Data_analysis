{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание 2 по ML\n",
        "Нужно написать python класс градиентного бустинга и побить другую модель на предоставленном baseline\n",
        "\n",
        "## Критерии оценки\n",
        "- Ваш ноутбук будет запущен через ```run all``` - он не должен упасть (допускается падение из-за отсутствия библиотеки, которую можно поставить через pip install)\n",
        "- Вот этот код (внизу ноутбука) ```assert imp_my_little_model > imp_baseline_model``` не вызвал ошибок (успешно отработал)\n",
        "\n",
        "- реализованы следующие гиперпараметры\n",
        "  - вы реализовали гиперпараметр ```learning_rate```\n",
        "  - вы реализовали гиперпараметр ```n_estimators```\n",
        "  - вы реализовали гиперпараметр ```max_depth```\n",
        "  - вы реализовали гиперпараметр ```bagging_fraction```\n",
        "\n",
        "- Вы реализовали [Huber loss function](https://ru.wikipedia.org/wiki/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F_%D0%BF%D0%BE%D1%82%D0%B5%D1%80%D1%8C_%D0%A5%D1%8C%D1%8E%D0%B1%D0%B5%D1%80%D0%B0) - она записана как отдельная def функция вне класса - и используется в вашем классе для расчета\n",
        "\n",
        "----\n",
        "*Для успешной сдачи дз нужно выполнить полностью каждый пункт выше*\n",
        "\n",
        "- оценка 5 будет поставлена, если каждый пункт выполнен без недочетов\n",
        "- оценка 4 будет поставлена, если будет найден один недочет\n",
        "- незачет будет пославлен, если недочетов будет два или более\n",
        "- незачет будет пославлен, если какой либо пункт не выполнен\n"
      ],
      "metadata": {
        "id": "Xj_j1Ibc6376"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "1YyKnF4T6e5Y",
        "outputId": "61677fa2-74d2-4741-b93b-3f1956a8465e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.notebook.set_autosave_interval(60000)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autosaving every 60 seconds\n"
          ]
        }
      ],
      "source": [
        "%autosave 60"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install shap\n",
        "\n",
        "import numpy as np\n",
        "import shap\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ],
      "metadata": {
        "id": "Fuou3iRh67G-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5748b1e-f267-4cbb-8157-8dd323e4f198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.45.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (538 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m538.2/538.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.0.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.2)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.0)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.45.0 slicer-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "TSWaYEYyT0iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, target = shap.datasets.california()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "M4B4Ep5U68x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Не меняйте название для предскзаний preds_my_little_model, иначе не получится сдать это ДЗ (сломается код)\n",
        "\n",
        "Некоторые правила\n",
        "- Нельзя использовать никакие другие алгоритмы моделей внутри вашего класса, кроме DecisionTreeRegressor.\n",
        "- Код вашего бустинга должен быть написан в классе, у класса должно быть два ожидаемых метода : ```fit``` и ```predict```.\n",
        "- Нельзя менять датасет (и модифицировать тоже, например заполнять nan или применять scaler) или baseline модель\n",
        "- Нельзя поднимать число n_estimators вашей модели выше 100 (чтобы результат был сравним с моделью-конкурентом ```GradientBoostingRegressor```)"
      ],
      "metadata": {
        "id": "A6-T1zHhWl69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *это место для вашего кода* ↓↓↓\n"
      ],
      "metadata": {
        "id": "61wEAl-UIA4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def huber_loss(y_true, y_pred, delta=1):\n",
        "  '''Функция для вычисления ГРАДИЕНТА функции ошибки huber loss (так как в оптимизации нужно считать именно градиент ошибки)'''\n",
        "  err = y_true - y_pred\n",
        "  return np.where(abs(err) <= delta, err, delta * np.sign(err))"
      ],
      "metadata": {
        "id": "5ahK530JP-TV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "W-8ynxW3Onr9"
      },
      "outputs": [],
      "source": [
        "class MyGradBoosting:\n",
        "    \"\"\"Класс, описывающий градиентный бустинг\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate=0.2,\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        random_state=0,\n",
        "        bagging_fraction=0.9,\n",
        "    ):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.random_state = random_state\n",
        "        self.bagging_fraction = bagging_fraction\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n = len(X)\n",
        "        self.trees = []   # Список обученных слабых моделей\n",
        "        f = np.zeros(n)  # Начальный прогноз модели\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            loss = huber_loss(y, f)  # Градиент функции ошибки\n",
        "            bagging_ind = np.random.choice(n, int(n * self.bagging_fraction), replace=True)  # Бэггинг записей\n",
        "\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth, random_state=self.random_state)\n",
        "            tree.fit(X.iloc[bagging_ind], loss[bagging_ind])\n",
        "\n",
        "            f +=  self.learning_rate * tree.predict(X)  # Пересчет прогноза модели\n",
        "            self.trees.append(tree)\n",
        "\n",
        "\n",
        "    def predict(self, samples):\n",
        "        n = len(samples)\n",
        "        predict = np.zeros(n) # Начальный прогноз\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            predict +=  self.learning_rate * self.trees[i].predict(samples)  # Пересчет прогноза\n",
        "\n",
        "        return predict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_little_model = MyGradBoosting()\n",
        "my_little_model.fit(X_train, y_train)\n",
        "\n",
        "preds_my_little_model = my_little_model.predict(X_test)"
      ],
      "metadata": {
        "id": "jDMV0_KfBDEd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# самопроверки\n",
        "assert preds_my_little_model.shape == y_test.shape, 'что-то не так с выходным размером предикта'"
      ],
      "metadata": {
        "id": "CPzhitJHKdXV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *это место для вашего кода* ↑↑↑"
      ],
      "metadata": {
        "id": "XekwIQWHIKtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *ниже ничего менять не нужно*\n",
        "## Это класс судья - он решит, какая модель оказалась лучше, ваша, или GradientBoostingRegressor из sklearn\n",
        "Если ячейка ниже завершилась ошибкой, нужно поменять код вашей модели и попробовать еще раз, до тех пор, пока не получите сообщение \"Ура, получилось!\""
      ],
      "metadata": {
        "id": "WrLezIsFHUP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = GradientBoostingRegressor(random_state=4, verbose=0)\n",
        "baseline_model = baseline_model.fit(X_train, y_train)\n",
        "preds_baseline_model = baseline_model.predict(X_test)\n",
        "print('mape - ваша модель', mean_absolute_percentage_error(y_test, preds_my_little_model))\n",
        "print('mape - baseline', mean_absolute_percentage_error(y_test, preds_baseline_model))\n",
        "\n",
        "final_estimator = RandomForestRegressor(random_state=16)\n",
        "final_estimator = final_estimator.fit(\n",
        "    np.hstack((preds_baseline_model.reshape(-1, 1), preds_my_little_model.reshape(-1, 1))),\n",
        "    y_test\n",
        ")\n",
        "\n",
        "imp_baseline_model, imp_my_little_model = final_estimator.feature_importances_\n",
        "result_message = f\"baseline важность: {imp_baseline_model:0.3f}; важность вашей модели: {imp_my_little_model:0.3f}\"\n",
        "\n",
        "assert imp_my_little_model > imp_baseline_model,  f'попробуй еще раз: {result_message}'\n",
        "print('Ура, получилось!',  result_message)"
      ],
      "metadata": {
        "id": "5cwE3WwPBcKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dc289a0-edf1-45a8-b0dd-f9278ad67420"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mape - ваша модель 0.18522226037284975\n",
            "mape - baseline 0.2152446498010688\n",
            "Ура, получилось! baseline важность: 0.091; важность вашей модели: 0.909\n"
          ]
        }
      ]
    }
  ]
}